{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model /root/data/ojt/output/debug_model_mlm/checkpoint_6000.pt\n",
      "128\n",
      "[[['an', 'so', 'q', 'be', '##king', '##v']]]\n",
      "[[['ha', 'to', '##e', '##all', 'no', '##er']]]\n",
      "[[['##us', 'i', 'p', '##pp', 'me', '##nt'], ['to', '##e', '##all', '##er', 'ha', 'a']]]\n",
      "[[['##t', 'place', 'you', '##i', 'to', 'where']]]\n",
      "[[['ne', 'where', 'know', '##pt', '##v', '##nt'], ['##w', 'the', '##ad', '##en', 'star', '##ch']]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 모델 불러와서 MLM Test \"\"\"\n",
    "\n",
    "from plm_trainer import PLMDataset\n",
    "import torch\n",
    "import json\n",
    "from model.bert import Bert\n",
    "from transformers import BertTokenizer\n",
    "import sys\n",
    "import utils\n",
    "from torch.utils.data import DataLoader\n",
    "import os \n",
    "\n",
    "def predict_mask_token(bert,text,with_cuda=False):\n",
    "    token_result = bert.tokenizer(text, is_split_into_words=True,max_length=bert.config[\"max_seq_len\"],padding=\"max_length\",return_token_type_ids=True)\n",
    "\n",
    "\n",
    "    data ={}\n",
    "\n",
    "    data[\"input_ids\"] = torch.tensor([token_result[\"input_ids\"]],dtype=torch.int) \n",
    "    data[\"seg_ids\"]=torch.tensor([token_result[\"token_type_ids\"]],dtype=torch.int)\n",
    "    data[\"att_masks\"]=torch.tensor([token_result[\"attention_mask\"]],dtype=torch.int)\n",
    "\n",
    "    mlm_positions=[]\n",
    "    mlm_masks=[]\n",
    "    for index,id in enumerate(token_result[\"input_ids\"]):\n",
    "        if id ==bert.vocab[utils.MASK_TOKEN]:\n",
    "            mlm_positions.append(index)\n",
    "            mlm_masks.append(1)\n",
    "\n",
    "    if len(mlm_positions)<bert.config[\"max_mask_tokens\"]:\n",
    "        pad_num = bert.config[\"max_mask_tokens\"]-len(mlm_positions)\n",
    "        mlm_positions.extend([0]*pad_num)\n",
    "        mlm_masks.extend([0]*pad_num)\n",
    "    \n",
    "    # [max_token_num]\n",
    "    data[\"mlm_positions\"]=torch.tensor([mlm_positions],dtype=torch.int)\n",
    "    data[\"mlm_masks\"]=torch.tensor([mlm_masks],dtype=torch.int)\n",
    "\n",
    "    \n",
    "    result,att_score_list = bert(data)\n",
    "\n",
    "    print(bert.convert_mask_pred_to_token(result[\"mask_pred\"],data[\"mlm_masks\"],top_k=6))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_path = \"/root/data/ojt/datasets/book_corpus_debug.txt\"\n",
    "model_dir = \"/root/data/ojt/output/debug_model_mlm\"\n",
    "model_name = \"checkpoint_6000.pt\"\n",
    "\n",
    "# train_path = \"/root/data/ojt/datasets/books_corpus_p1_1.txt\"\n",
    "# model_dir = \"/root/data/ojt/output/bert_small_book_1_mlm_sop_lr\"\n",
    "# model_name = \"checkpoint_106000.pt\"\n",
    "\n",
    "config_path = os.path.join(model_dir,utils.MODEL_CONFIG_NAME)\n",
    "vocab_file_path = os.path.join(model_dir,utils.VOCAB_TXT_FILE_NAME)\n",
    "model_path = os.path.join(model_dir,model_name)\n",
    "\n",
    "\n",
    "with open(config_path,\"r\") as cfg_json:\n",
    "    config = json.load(cfg_json)\n",
    "\n",
    "tokenizer=BertTokenizer(vocab_file=vocab_file_path,do_lower_case=True)\n",
    "\n",
    "bert=Bert(config=config,tokenizer=tokenizer,with_cuda=False,return_mlm=True,return_sop=False)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print(\"Loading model %s\"%model_path)\n",
    "bert.load_state_dict(torch.load(model_path,map_location=device))\n",
    "bert.eval()\n",
    "\n",
    "print(config[\"max_seq_len\"])\n",
    "id_to_vocab = {v:k for k,v in tokenizer.vocab.items()}\n",
    "plm_dataset=PLMDataset(train_path,tokenizer,128,config[\"max_seq_len\"],config[\"max_mask_tokens\"],cached_dir=model_dir,use_cache=False,mlm_data=True)\n",
    "plm_data_loader = DataLoader(plm_dataset, batch_size=1,num_workers=1)\n",
    "\n",
    "\"\"\" 학습 데이터들에 대한 예측 결과 보기 \"\"\"\n",
    "# for i,data in enumerate(plm_data_loader):\n",
    "#     # print(data)\n",
    "#     input_ids=data[\"input_ids\"].view(-1).tolist()\n",
    "#     # for i in input_ids:\n",
    "#     #     print(id_to_vocab[i],end=\" \")\n",
    "#     mask_labels=data[\"mlm_labels\"].view(-1).tolist()\n",
    "#     for i in mask_labels:\n",
    "#         print(id_to_vocab[i],end=\" \")\n",
    "#     result,att_score_list = bert(data)\n",
    "#     print(\"\")\n",
    "#     print(bert.convert_mask_pred_to_token(result[\"mask_pred\"],data[\"mlm_masks\"],top_k=1))\n",
    "#     print(\"\")\n",
    "\n",
    "\n",
    "\"\"\" 입력한 Text에 대한 예측 결과 보기 \"\"\"\n",
    "#i wish i had a better answer to that question\n",
    "text = \"i wish i had a better [MASK] to that question . starlings , new york is not the place youd expect much to happen .\"\n",
    "predict_mask_token(bert,text)\n",
    "text = \"i wish i [MASK] a better answer to that question . starlings , new york is not the place youd expect much to happen .\"\n",
    "predict_mask_token(bert,text)\n",
    "text = \"i wish [MASK] had a better answer [MASK] that question . starlings , new york is not the place youd expect much to happen .\"\n",
    "predict_mask_token(bert,text)\n",
    "text = \"i wish i had a better answer to that question . starlings , new york is not the [MASK] youd expect much to happen .\"\n",
    "predict_mask_token(bert,text)\n",
    "text = \"i wish i had a better answer to that question . starlings , [MASK] [MASK] york is not the place youd expect much to happen .\"\n",
    "predict_mask_token(bert,text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pkl file = /root/data/ojt/datasets/book_corpus_debug.pkl\n",
      "doc num = 1, sentence num = 31\n",
      "{'input_ids': tensor([[  2,  26, 163,  53,   4,  26, 129,  48,  18, 100,  58,  58,  79,  85,\n",
      "          53,  52,  79,  86,   4,  34,  60, 124,  58, 133,   7,   3, 127,  44,\n",
      "         174,   5, 158,  52,  42, 140,  64, 118,   4,  58,  78, 188,  99,  48,\n",
      "          22,  65,   4,  47,   4,  58,  30,   4, 143,  86, 129, 122, 111,   4,\n",
      "           3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  2, 106,   4, 100, 171, 169,  47, 116, 115,  23,   4,  44,  58, 131,\n",
      "          36,  49, 174,  47,   5, 131,  20, 141,  72,  55,   7,  78,   4, 153,\n",
      "          92, 150,  78, 129,  44,  72, 119,  90,   4,   3, 128,  58,  26, 153,\n",
      "           7,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  2,  26,  28,  45, 172,  80,  47, 161, 172, 187,  78,  24,  57, 112,\n",
      "           4,   7,   3,  78, 120,  50,   4, 127, 142,  86,   1,  58,   4,  33,\n",
      "         138, 142,   4,   4,  55, 155,  83,  57,  45,  91,  45, 135,  58,  32,\n",
      "          57, 135,  54, 124,  93, 107, 166,   7,   3,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  2,  26,  76,  60,  61,  62,   4,  78, 157,  55,  46,  87,   4,  32,\n",
      "          46, 111,  80,   4, 101,  53,  53, 167,  47, 184, 129,  48,   1,  92,\n",
      "         101,   4,   3,  90, 107,  87,  16, 109,   4, 143,  49, 153,  81,  17,\n",
      "           3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0]], dtype=torch.int32), 'seg_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32), 'att_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32), 'mlm_labels': tensor([[ 59,  18, 126, 159,  46,  63,  60,   7,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [121,  47,  72,  72,  55,   7,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [ 18,  53, 104,   5,  78, 162,  19,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [ 80,  93,  78,   7,  90,  96,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0]]), 'mlm_positions': tensor([[ 4,  8, 18, 36, 44, 46, 49, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [ 2, 10, 16, 22, 26, 36,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [ 6, 14, 20, 26, 30, 31, 34,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [ 6, 12, 17, 29, 31, 36,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0]], dtype=torch.int32), 'mlm_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       dtype=torch.int32), 'sop_labels': tensor([1, 1, 1, 1])}\n",
      "[CLS] [M]i wi [M]##s ##h i [M]ha ##d a be ##t ##t ##er an ##s ##w ##er to that q ##u ##es ##t ##ion . [SEP] star ##l [M]##ings , ne ##w y ##or [M]##k is no ##t [M]the place you ##d e ##x ##p ##e ##c ##t m ##u [M]##ch to [M]ha ##pp ##en . [SEP] \n",
      "[CLS] on ##ly be ##ca ##us [M]##e every ##one f ##e ##l ##t [M]so s [M]##a ##f ##e [M], [M]so c ##om ##f ##y . [SEP] the ##y do ##nt know the ha ##l [M]##f of it . bu ##t i do . [SEP] \n",
      "[CLS] i k [M]##i [M]##ck [M]##ed a ro ##ck into the g ##r ##as ##s . [SEP] the su [M]##n was [M]star [M]##ting to [UNK] ##t , p ##ain ##ting the sk [M]##y in b ##r ##i ##ll ##i ##an ##t o ##r ##an ##g ##es and re ##ds . [SEP] \n",
      "[CLS] i th ##u ##m ##b ##ed the [M]ke [M]##y ##p ##ad and o ##p [M]##en [M]##ed the me ##s ##s ##ag ##e seth ha ##d [UNK] ##nt [M]me [M]. [SEP] it re ##ad : wh ##at ##ch ##a do ##ing ? [SEP] \n"
     ]
    }
   ],
   "source": [
    "\"\"\" 데이터 제작 확인 \"\"\"\n",
    "from transformers import BertTokenizer\n",
    "from plm_dataset import PLMDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "vocab_file_path = \"/root/data/ojt/output/debug_model/vocab.txt\"\n",
    "train_path = \"/root/data/ojt/datasets/book_corpus_debug.txt\"\n",
    "\n",
    "tokenizer=BertTokenizer(vocab_file=vocab_file_path,do_lower_case=True)\n",
    "plm_dataset=PLMDataset(train_path,tokenizer,data_max_seq_len=64,model_max_seq_len=128,max_mask_tokens=20)\n",
    "train_data_loader = DataLoader(plm_dataset, batch_size=4,num_workers=1)\n",
    "\n",
    "vocab = tokenizer.vocab\n",
    "id_to_vocab={v:k for k,v in vocab.items()}\n",
    "\n",
    "print(iter(train_data_loader).next())\n",
    "\n",
    "for data in train_data_loader:\n",
    "    input_ids=data[\"input_ids\"].tolist()\n",
    "    mlm_positions=data[\"mlm_positions\"].tolist()\n",
    "    mlm_labels=data[\"mlm_labels\"].tolist()\n",
    "\n",
    "     \n",
    "    for i,seq in enumerate(input_ids):\n",
    "        mlm_label_index=0\n",
    "        # print(mlm_positions[i])\n",
    "        for j,token_id in enumerate(seq):\n",
    "            \n",
    "\n",
    "            if j!=0 and j in mlm_positions[i]:\n",
    "            \n",
    "                \n",
    "                print(\"[M]\"+id_to_vocab[mlm_labels[i][mlm_label_index]],end=\" \")\n",
    "                mlm_label_index+=1\n",
    "            elif token_id!=0:\n",
    "                print(id_to_vocab[token_id],end=\" \")\n",
    "      \n",
    "\n",
    "        print(\"\")\n",
    "    \n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
